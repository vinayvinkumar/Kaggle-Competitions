{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition - \"Titanic: Machine Learning from Disaster\"\n",
    "   ## by Vinay Kumar Ranganath Babu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all the required libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the data to a DataFrame\n",
    "titanic = pd.read_csv(\"titanic_train.csv\")\n",
    "\n",
    "# Print the first 5 rows of the dataframe.\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data LookUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>FamilyId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.361582</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>0.904602</td>\n",
       "      <td>26.965208</td>\n",
       "      <td>14.232323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>13.019697</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>1.613459</td>\n",
       "      <td>9.281607</td>\n",
       "      <td>69.886368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>633.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
       "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  FamilySize  NameLength    FamilyId  \n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  \n",
       "mean     0.381594   32.204208    0.904602   26.965208   14.232323  \n",
       "std      0.806057   49.693429    1.613459    9.281607   69.886368  \n",
       "min      0.000000    0.000000    0.000000   12.000000   -1.000000  \n",
       "25%      0.000000    7.910400    0.000000   20.000000   -1.000000  \n",
       "50%      0.000000   14.454200    0.000000   25.000000   -1.000000  \n",
       "75%      0.000000   31.000000    1.000000   30.000000   -1.000000  \n",
       "max      6.000000  512.329200   10.000000   82.000000  633.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all the missing values in the Age column\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\python35-32\\lib\\site-packages\\ipykernel\\__main__.py:8: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting using Logistic Regression (for Training Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing The Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to do all the cleaning and pre-requsite settings even for the test set\n",
    "titanic_test = pd.read_csv(\"titanic_test.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns PassengerID and Survived\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         1\n",
       "19           911         1\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         1\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         1\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         1\n",
       "404         1296         0\n",
       "405         1297         0\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         1\n",
       "409         1301         1\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         1\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submission can be viewed as below\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression gives us the accuracy of 75%. We can improve this using Ensembling methods of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling using Random Forests and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78563411896745228"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing A Random Forest\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the accuracy of 78.5%. We can further improve this by using parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81593714927\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = cross_validation.KFold(titanic.shape[0], 3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is now improved to 81.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using The 'Title' field of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Mlle          2\n",
      "Major         2\n",
      "Col           2\n",
      "Mme           1\n",
      "Countess      1\n",
      "Ms            1\n",
      "Sir           1\n",
      "Capt          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Lady          1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Family Groups from our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pd.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAE2CAYAAACqSMMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOcaaHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5e81330>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817059483726\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\", \"NameLength\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting and Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\python35-32\\lib\\site-packages\\ipykernel\\__main__.py:37: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is improved to ~82%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Our Changes On The Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n",
      "{'Dahl0': 299, 'Moraweck0': 285, 'Mockler0': 315, 'Drew2': 358, 'Dimic0': 306, 'Windelov0': 417, 'Zabour1': 108, 'Bengtsson0': 152, 'Gustafsson0': 331, 'Hodges0': 586, 'Sunderland0': 202, 'Mitkoff0': 533, 'Pasic0': 666, 'Karaic0': 504, 'Campbell0': 401, 'Sirayanian0': 60, 'Gaskell0': 637, 'Milling0': 398, 'Lester0': 651, 'Peters0': 557, 'Bailey0': 611, 'Nicholls2': 136, 'Sadlier0': 338, 'Persson1': 241, 'Roebling0': 686, 'Harder1': 324, 'Cribb1': 150, 'Mellinger1': 246, 'Porter0': 107, 'Smart0': 402, 'Lovell0': 209, 'Reuchlin0': 660, 'Chronopoulos1': 71, 'Moss0': 104, 'Mack0': 623, 'Jussila1': 110, 'Caram1': 482, \"O'Sullivan0\": 426, 'Lindqvist1': 543, 'Vander Planke2': 38, 'Lennon1': 46, 'Nye0': 66, 'Uruchurtu0': 30, 'Leitch0': 496, 'Long0': 632, 'Jenkin0': 69, 'Adams0': 346, 'Webber0': 117, 'Hippach1': 294, 'Sutton0': 516, 'Arnold-Franchi1': 49, 'Parkes0': 251, 'Clifford0': 408, 'Harknett0': 214, 'Vande Velde0': 608, 'Madigan0': 181, 'Baxter1': 114, 'Emanuel0': 628, 'Renouf1': 409, 'Butt0': 451, 'Nenkoff0': 205, 'Mamee0': 36, 'Chaffee1': 89, 'Cardeza1': 556, 'Hosono0': 260, 'Goncalves0': 400, 'Bazzani0': 200, 'Ilett0': 82, 'Christy2': 484, 'Jensen0': 527, 'Slemen0': 652, 'Wiklund1': 325, 'Moran0': 6, 'Brocklebank0': 509, 'Chapman0': 568, 'Carr0': 190, 'Hassab0': 558, 'Myhrman0': 626, 'Carter3': 340, 'Johnson0': 273, 'Leader0': 641, 'Lesurer0': 596, 'Ohman0': 465, 'Sagesser0': 528, 'Artagaveytia0': 419, 'Laleff0': 691, 'Burke0': 134, 'Cor0': 530, 'LeRoy0': 452, 'Haas0': 265, 'Andreasson0': 88, 'Fleming0': 275, 'Spedden2': 287, 'Lefebre4': 162, 'Vestrom0': 15, 'Lang0': 431, 'Cacic0': 405, 'Johanson0': 184, 'Davis0': 525, 'Canavan0': 425, 'Matthews0': 360, 'Montvila0': 698, 'Mitchell0': 550, 'Giglio0': 130, 'Sheerlinck0': 79, 'Douglas1': 457, 'Gilinski0': 490, 'Newell1': 197, 'Ali0': 192, 'Backstrom3': 83, 'Hamalainen2': 224, 'Greenfield1': 94, 'Woolner0': 55, 'Pears1': 141, 'de Mulder0': 258, 'Shelley1': 693, 'Hart2': 283, 'Beane1': 456, 'Pinsky0': 174, 'de Messemaeker1': 469, 'Coelho0': 123, 'Fynney0': 21, 'Green0': 204, 'Barber0': 262, 'Frost0': 412, 'Van Impe2': 361, 'Samaan2': 48, 'Bishop1': 263, 'de Pelsmaeker0': 255, 'Oreskovic0': 347, 'Romaine0': 171, 'Baclini3': 384, 'Leeni0': 464, 'Abelson1': 277, 'Vanden Steen0': 311, 'Daly0': 432, 'Carter1': 226, 'Andrew0': 135, 'Rosblom2': 231, 'Coleff0': 435, 'Natsch1': 247, 'Mineff0': 266, 'Slocovski0': 85, 'Maisner0': 399, 'Stewart0': 64, 'Ivanoff0': 597, 'Vander Planke1': 19, 'Pavlovic0': 440, 'Foo0': 529, 'Albimona0': 189, 'Davies2': 460, 'Hirvonen1': 411, 'Jansson0': 341, 'Keefe0': 404, 'Crosby2': 455, 'Aks1': 678, 'Gilnagh0': 146, 'Jacobsohn3': 498, 'Farrell0': 445, 'Petterson1': 379, 'Pengelly0': 217, 'Asplund6': 25, 'Bateman0': 140, 'Frolicher-Stehli2': 489, 'Farthing0': 447, 'Troutt0': 582, 'Sivola0': 159, 'Johnston3': 633, 'Kelly0': 271, 'Wick2': 286, 'Newsom2': 128, 'Weir0': 567, 'Lam0': 565, 'Trout0': 344, \"O'Brien1\": 170, 'Nysveen0': 292, 'Padro y Manent0': 459, 'Meanwell0': 474, 'Parr0': 524, 'Lobb1': 230, 'Louch1': 373, 'Marechal0': 669, 'Betros0': 330, 'Strom1': 187, 'Dahlberg0': 695, 'Bystrom0': 684, 'Anderson0': 395, 'Robbins0': 468, 'Barton0': 109, 'Ball0': 293, 'Silverthorne0': 572, 'Nilsson0': 284, 'Andrews0': 647, 'Yousif0': 310, 'Harrington0': 500, 'Smiljanic0': 148, 'Kent0': 415, 'Lahtinen2': 281, 'Bracken0': 203, 'Peuchen0': 385, 'Pickard0': 370, 'Taussig2': 237, 'Thomas1': 645, 'Cohen0': 186, 'Harris1': 62, 'Shorney0': 92, 'Penasco y Castellana1': 276, 'Reed0': 227, 'Ling0': 157, 'Potter1': 692, 'Stanley0': 420, 'Sdycoff0': 352, 'Rugg0': 56, 'White1': 99, 'Bissette0': 243, 'Moran1': 106, 'Wells2': 606, 'Badt0': 541, 'Paulner0': 487, 'Buss0': 337, 'Hagland1': 386, 'Christmann0': 87, 'Silvey1': 375, 'Lines1': 677, 'Jensen1': 584, 'Tikkanen0': 334, 'Gale1': 348, 'West3': 58, 'Hold1': 215, 'Stoytcheff0': 475, 'Hocking3': 449, 'Kvillner0': 377, 'Stankovic0': 257, 'Keane0': 274, 'Abbing0': 675, 'Daniel0': 505, 'Longley0': 518, 'Leyson0': 213, 'Hampe0': 378, 'Connaghton0': 605, 'Najib0': 690, 'Williams1': 145, 'Parrish1': 236, 'Hocking4': 625, 'Devaney0': 44, 'Doharr0': 476, 'Givard0': 195, 'Harmer0': 634, 'Gee0': 397, 'Yousseff0': 421, 'Somerton0': 416, 'Astor1': 571, 'Laitinen0': 427, 'Widegren0': 349, 'Markoff0': 676, 'Fahlstrom0': 210, 'Nosworthy0': 51, 'Robins1': 124, 'Jacobsohn1': 199, 'Sedgwick0': 302, 'Brown0': 177, 'Gill0': 683, 'Cleaver0': 576, 'Francatelli0': 278, 'Cairns0': 244, 'Strandberg0': 407, 'Sirota0': 667, 'Dean3': 90, 'Olsen0': 144, 'Landergren0': 328, 'Mayne0': 577, 'Klasen2': 161, 'Hickman2': 115, 'Collander0': 301, 'Rouse0': 413, 'Mellors0': 208, 'Tobin0': 627, 'Calic0': 153, 'Johnson2': 9, 'Graham1': 242, 'Jussila0': 483, 'Backstrom1': 188, 'Moore0': 116, 'Karlsson0': 410, \"O'Brien0\": 463, 'Frauenthal1': 296, 'Mernagh0': 179, 'Tornquist0': 245, 'Reeves0': 240, 'Danbom2': 365, 'Radeff0': 537, 'Olsen1': 180, 'Sloper0': 24, 'Hansen1': 574, 'Healy0': 248, 'Harper1': 52, 'Davidson1': 549, 'Andrews1': 249, 'Allen0': 5, 'McGowan0': 23, 'Walker0': 436, 'Richards2': 350, 'Abbott2': 252, 'Richards5': 376, 'Eustis1': 422, 'Holm0': 657, 'Hedman0': 646, 'Willey0': 532, 'Saalfeld0': 270, 'Coutts2': 305, 'Rood0': 169, 'Attalah0': 111, 'McCormack0': 661, 'Sutehall0': 697, 'Hawksford0': 599, 'Pain0': 343, 'Sundman0': 356, 'Futrelle1': 4, 'Bradley0': 430, \"O'Driscoll0\": 47, 'Blackwell0': 300, 'Humblen0': 570, 'Lurette0': 178, 'Duane0': 253, 'Navratil2': 138, 'Lundahl0': 522, 'Quick2': 429, 'Bjornstrom-Steffansson0': 371, 'Theobald0': 612, \"O'Connell0\": 520, 'Elias2': 309, 'Cameron0': 193, 'Shellard0': 423, 'Petranec0': 97, 'Nankoff0': 598, 'Rekic0': 105, 'Phillips0': 368, 'Gronnestad0': 621, 'Barbara1': 317, 'Greenberg0': 579, 'Kimball1': 513, 'Ibrahim Shawah0': 643, 'Calderhead0': 575, 'Eklund0': 617, 'Kink2': 68, 'Moor1': 607, 'Cavendish1': 600, 'Thorneycroft1': 372, 'Maenpaa0': 221, 'Balkic0': 688, 'Graham0': 699, 'Warren1': 320, 'McDermott0': 80, 'Dooley0': 701, 'Bourke2': 172, 'Soholt0': 580, 'Aubart0': 323, 'Markun0': 694, 'Sinkkonen0': 603, 'Ayoub0': 631, 'Flynn0': 369, 'Masselmani0': 20, 'Bidois0': 332, 'Holverson1': 35, 'Turkula0': 414, 'Morley0': 396, 'Klaber0': 578, 'Lindell1': 503, 'Ringhini0': 327, 'Lemore0': 437, 'Palsson4': 8, 'Hays0': 279, 'Dorking0': 256, 'Goldschmidt0': 93, 'Lindahl0': 223, 'Andersson0': 137, 'Torber0': 501, 'Meyer1': 34, 'Kraeff0': 42, 'Harris0': 201, 'Ford4': 84, 'Olsson0': 254, 'Wheadon0': 33, 'Allison3': 269, 'Funk0': 313, 'Stahelin-Maeglin0': 523, 'Caldwell2': 76, 'Leinonen0': 526, 'Madill1': 562, 'Toomey0': 393, 'Jermyn0': 322, 'Molson0': 418, 'Gallagher0': 573, 'Kink-Heilmann2': 168, \"O'Connor0\": 394, 'Vovk0': 442, 'Asim0': 318, 'Lemberopolous0': 673, 'Madsen0': 119, 'Gavey0': 511, 'Chibnall1': 155, 'Spencer1': 31, 'Ponesell0': 644, 'Sage10': 149, 'Hogeboom1': 618, 'Beesley0': 22, 'Duff Gordon1': 467, 'Otter0': 640, 'Crease0': 67, 'Clarke1': 367, 'Niskanen0': 345, 'Lulic0': 659, 'Waelens0': 78, 'Harrison0': 238, 'Appleton2': 478, 'Berriman0': 594, 'Elsbury0': 494, 'Rommetvedt0': 545, 'Laroche3': 43, 'Meek0': 357, 'Nicholson0': 458, 'Williams-Lambert0': 308, 'Fortune5': 27, 'Staneff0': 74, 'Hoyt1': 206, 'Birkeland0': 351, 'Celotti0': 86, 'Vande Walle0': 183, 'Fry0': 654, 'Bowerman1': 312, 'Plotcharsky0': 335, 'Jonkoff0': 609, 'del Carlo1': 316, 'Perreault0': 441, 'Collyer2': 216, 'Hale0': 164, 'Ward0': 235, 'Thayer2': 461, 'Kassem0': 444, 'Van der hoef0': 158, 'Ross0': 486, 'Nicola-Yarred1': 39, 'Goodwin7': 59, 'Carbines0': 175, 'Kallio0': 374, 'Ridsdale0': 446, 'Colley0': 542, 'Dowdell0': 77, 'Boulos2': 131, 'Moutal0': 75, 'Tomlin0': 653, 'Hoyt0': 638, 'Thorne0': 233, 'Hansen0': 514, 'Bostandyeff0': 519, 'Hegarty0': 536, 'McKane0': 342, 'Adahl0': 319, 'Jalsevac0': 390, 'McNamee1': 601, 'Guggenheim0': 636, 'Sivic0': 471, 'Hakkarainen1': 133, 'Davies0': 336, 'van Melkebeke0': 687, 'Rogers0': 45, 'Henry0': 239, 'Robert1': 630, 'van Billiard2': 143, 'Andersson6': 14, 'Edvardsson0': 553, 'Lewy0': 267, 'Olsvigen0': 559, 'Watson0': 552, 'Slayter0': 290, 'Rothschild1': 434, 'Nirva0': 615, 'Cumings1': 2, 'Dantcheff0': 639, 'Emir0': 26, 'Gustafsson2': 101, 'Downton0': 485, 'Jardin0': 507, 'Levy0': 264, 'Risien0': 453, 'Doling1': 95, 'Seward0': 383, 'Moubarek2': 65, 'Barkworth0': 521, 'Yasbeck1': 512, 'Strom2': 228, 'Alexander0': 650, 'Angle1': 439, 'Shutes0': 506, 'Honkanen0': 198, 'Cherry0': 234, 'Stead0': 229, 'Bing0': 72, 'Pettersson0': 648, 'Jerwan0': 406, 'McCoy2': 272, 'Jonsson0': 477, 'Chambers1': 587, 'Augustsson0': 663, 'Troupiansky0': 595, 'Widener2': 329, \"O'Dwyer0\": 28, 'Fox0': 303, 'Patchett0': 480, 'Behr0': 700, 'Razi0': 679, 'Minahan2': 222, 'Hansen2': 680, 'Rice5': 17, 'Young0': 291, 'Foreman0': 387, 'Ilmakangas1': 591, 'Coleridge0': 220, 'Swift0': 682, 'Nakid2': 333, 'Osen0': 129, 'Blank0': 191, 'Nasser1': 10, 'Mallet2': 656, 'Naidenoff0': 259, 'Kilgannon0': 629, 'Garfirth0': 614, 'Lehmann0': 339, 'Rothes0': 613, 'Moussa0': 321, 'Byles0': 139, 'Coxon0': 91, 'Hanna0': 268, 'Frolicher2': 454, 'Denkoff0': 297, 'Chip0': 668, 'Richard0': 127, 'Duran y More1': 685, 'Dodge2': 382, 'Meyer0': 649, 'Yrois0': 182, 'Bowen0': 515, 'Hood0': 70, 'Berglund0': 207, 'Maioni0': 428, 'Murphy1': 219, 'Leonard0': 165, 'Burns0': 298, 'Morrow0': 470, 'Endres0': 581, 'Cook0': 546, 'Youseff0': 185, 'Peter2': 120, 'Wiseman0': 366, 'McEvoy0': 583, 'Ryerson4': 280, 'Braund1': 1, 'Ahlin1': 40, 'Turja0': 555, 'Johansson0': 100, 'Johannesen-Bratthammer0': 381, 'Faunthorpe1': 53, 'Kiernan1': 196, 'Horgan0': 508, 'Heininen0': 655, 'Carrau0': 81, 'Lindblom0': 250, 'Reynaldo0': 380, 'Stephenson1': 493, 'Compton2': 665, 'Renouf3': 588, 'Sobey0': 126, 'Kantor1': 96, 'Novel0': 57, 'Larsson0': 211, 'Norman0': 472, 'Heikkinen0': 3, 'Beckwith2': 225, 'Touma2': 232, 'Sjoblom0': 635, 'Panula5': 50, 'Hays2': 658, 'Turpin1': 41, 'Hunt0': 218, 'Meo0': 142, 'Dick1': 563, 'Gheorgheff0': 362, 'Svensson0': 424, 'Sandstrom2': 11, 'Mannion0': 589, 'Peduzzi0': 389, 'Smith0': 160, 'Lievens0': 622, 'Stone0': 662, 'Barah0': 616, 'Todoroff0': 29, 'Salkjelsvik0': 103, 'Watt0': 151, 'Pernot0': 166, 'Danoff0': 289, 'Hewlett0': 16, 'Davison1': 304, 'Frauenthal2': 540, 'Kirkland0': 517, 'McMahon0': 118, 'Perkin0': 194, 'Simonius-Blumer0': 531, 'Petroff0': 98, 'Kalvik0': 534, 'Hassan0': 592, 'Mullens0': 569, 'Slabenoff0': 499, 'Boulos0': 497, 'Beavan0': 326, 'Odahl0': 307, 'Sawyer0': 554, 'Hart0': 353, 'Mionoff0': 102, 'Bonnell0': 12, 'Taylor1': 547, \"O'Leary0\": 535, 'Stranden0': 602, 'Elias0': 624, 'Simmons0': 473, 'Drazenoic0': 122, 'Millet0': 391, 'McGovern0': 314, 'Rintamaki0': 492, 'Dennis0': 288, 'Skoog5': 63, 'Knight0': 593, 'Wright0': 466, 'Gillespie0': 585, 'Wilhelms0': 551, 'Pekoniemi0': 112, 'Osman0': 642, 'Fischer0': 561, 'Connors0': 113, 'Glynn0': 32, 'Marvin1': 604, 'Herman3': 510, 'Homer0': 502, 'Ekstrom0': 121, 'Isham0': 163, 'Mangan0': 620, 'Ryan0': 438, 'Kenyon1': 392, 'Weisz1': 125, 'Saad0': 566, 'Garside0': 481, 'Dakic0': 560, 'Becker3': 167, 'Saundercock0': 13, 'Eitemiller0': 538, 'Alhomaki0': 670, 'Banfield0': 696, 'Sjostedt0': 212, 'Vander Cruyssen0': 689, 'Butler0': 544, 'Moen0': 73, 'Connolly0': 261, 'Zimmerman0': 364, 'Allum0': 664, 'Charters0': 363, 'Salonen0': 448, 'Scanlan0': 403, 'Sharp0': 462, 'Icard0': 61, 'Carlsson0': 610, 'Chapman1': 495, 'Toufik0': 450, 'Turcin0': 173, 'Nysten0': 132, 'Goldenberg1': 388, 'Rush0': 479, 'Silven2': 359, 'Giles1': 681, 'Ostby1': 54, 'Goldsmith2': 154, 'Brown2': 548, 'Hendekovic0': 282, 'Baumann0': 156, 'Murdlin0': 491, 'Culumovic0': 674, 'Corn0': 147, 'Partner0': 295, 'Bryhl1': 590, 'Cann0': 37, 'Williams0': 18, 'McGough0': 433, 'Karun1': 564, 'McCarthy0': 7, 'Andersen-Jensen1': 176, 'Serepeca0': 672, 'Newell2': 539, 'Minahan1': 354, 'Cunningham0': 355, 'Brewe0': 619, 'Lahoud0': 443, 'Jarvis0': 488, 'Mudd0': 671}\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pd.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting On The Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting final results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
